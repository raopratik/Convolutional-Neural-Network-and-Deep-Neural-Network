{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pratik/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:34: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "# Imports\n",
    "from IPython.core.display import display, HTML\n",
    "from IPython.display import Image\n",
    "%matplotlib inline\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from nltk.tokenize import word_tokenize\n",
    "import tensorflow as tf\n",
    "import ast\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# WELCOME TO LSTM MODEL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**LSTMs:**\n",
    "\n",
    "This another popular approach. It does well with short as well as long sentences. It has various architectures which can be used. \n",
    "\n",
    "**Embeddings:**\n",
    "**Word embeddings** can be used. One of the major issues with word embeddings is out of vocabulary words and the memory required to store them. (Especially in cases of entities such as ‘person_name’). Embeddings are created using gensim library.\n",
    "\n",
    "**Character embeddings** can be used to solve the above problem in which each word is represented by using an embedding layer with one hot representations as input followed by convolutions on them.\n",
    "\n",
    "**Pretrained Embeddings** can be used. This approach leverages us the capability of transfer learning. These embeddings are trained on wikipedia or various other large generic corpuses hence have a greater insight into the language. We can make use of this insight from huge corpuses just by incorporating the embeddings\n",
    "\n",
    "\n",
    "**CRF Layer:**\n",
    "\tAn additional linear chain CRF can be added at the end of all the layers.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_SEQUENCE_LENGTH = 60\n",
    "MAX_CHAR_LENGTH = 16\n",
    "LSTM_UNITS = 64\n",
    "NUM_CLASSES = 2\n",
    "EPOCHS = 5\n",
    "NUM_DIMENSIONS = 100 \n",
    "BATCH_SIZE = 100\n",
    "TRAIN_LENGTH = 9000\n",
    "LSTM_LAYERS = 2\n",
    "LAYER_PARAMETERS = {'word': True, 'char': False, 'crf': True}\n",
    "TEST_BATCHES = 10\n",
    "\n",
    "\n",
    "SAVE_MODEL = 'models/malubu_lstm_context.ckpt'\n",
    "RAW_DATA_PATH = 'context_data.csv'\n",
    "WORD_VECTORS_PATH = 'gensim_embeddings_malubu'\n",
    "WORDS_LIST_PATH = 'gensim_embeddings_malubu_words'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### MAX_SEQUENCE_LENGTH = The maximum number of tokens in a given sentence\n",
    "##### MAX_CHAR_LENGTH = Maximum character length of each token\n",
    "##### LSTM_UNITS = Number of forward and backward lstm units\n",
    "##### NUM_CLASSES = Number of output classes\n",
    "##### EPOCHS = Number of epochs\n",
    "##### NUM_DIMENSIONS = Number of Dimensions of the word embeddings \n",
    "##### BATCH_SIZE = The size of the batches\n",
    "##### TRAIN_LENGTH = The length of the training set(Here we have cons.idered 2500 out of 3500)\n",
    "##### LSTM_LAYERS = The number of Deep LSTM layers\n",
    "##### LAYER_PARAMETERS = It allows you to switch on and of the 1.word_embedding layer 2.character_embedding  layer 3. crf layer\n",
    "##### TEST_BATCHES = Number of Test Batches\n",
    "##### SAVE_MODEL = The path where you wish to save your model\n",
    "##### RAW_DATA_PATH = The path of the data\n",
    "##### WORD_VECTORS_PATH = The path where your word vectors are stored\n",
    "##### WORDS_LIST_PATH = The path where your vocab is stored\n",
    "\n",
    "**NOTE: These are the only variables you are allowed and sufficient to change in this NOTEBOOK. Do not change any other parameters**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----------------------------------------------------------------------------------------------------\n",
    "### LSTM CLASS\n",
    "This is the main class which we will be using to train test and analyize our model. This is a fairly long piece of code for one class as a lot of preprocessing is required to get data into the lstm model and data visualization.\n",
    "\n",
    "The architecture shown above is implemented in the class along with customization options."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "from IPython.core.display import display, HTML\n",
    "from IPython.display import Image\n",
    "%matplotlib inline\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from nltk.tokenize import word_tokenize\n",
    "import tensorflow as tf\n",
    "import ast\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import re\n",
    "\n",
    "\n",
    "\n",
    "class CrfLstmModel(object):\n",
    "\n",
    "    def __init__(self, max_sequence_length, max_char_length, lstm_units, num_classes, epochs,\n",
    "                 num_dimensions, batch_size, train_len, lstm_layers, layer_parameters,\n",
    "                 test_batches, save_model, raw_data_path, word_vectors_path, words_list_path):\n",
    "        self.max_sequence_length = max_sequence_length\n",
    "        self.max_char_length = max_char_length\n",
    "        self.char_vocab = []\n",
    "        self.lstm_units = lstm_units\n",
    "        self.num_classes = num_classes\n",
    "        self.epochs = epochs\n",
    "        self.num_dimensions = num_dimensions\n",
    "        self.batch_size = batch_size\n",
    "        self.train_len = train_len\n",
    "        self.lstm_layers = lstm_layers\n",
    "        self.layer_parameters = layer_parameters\n",
    "        self.test_batches = test_batches\n",
    "\n",
    "        self.word_vectors_path = word_vectors_path\n",
    "        self.words_list_path = words_list_path\n",
    "        self.save_model = save_model\n",
    "        self.raw_data_path = raw_data_path\n",
    "\n",
    "        self.raw_data = None\n",
    "        self.X_train_body = None\n",
    "        self.y_train = None\n",
    "        self.X_test_body = None\n",
    "        self.y_test = None\n",
    "        self.ids_char_train_body = None\n",
    "        self.ids_char_test_body = None\n",
    "\n",
    "        self.X_train_context = None\n",
    "        self.y_train = None\n",
    "        self.X_test_context = None\n",
    "        self.y_test = None\n",
    "        self.ids_char_train_context = None\n",
    "        self.ids_char_test_context = None\n",
    "\n",
    "        self.fin_pred = None\n",
    "\n",
    "    def load_external_word_embeddings(self, word_vectors_path, words_list_path):\n",
    "        file_handler = open(self.word_vectors_path, 'rb')\n",
    "        word_vectors = pickle.load(file_handler)\n",
    "        file_handler = open(self.words_list_path, 'rb')\n",
    "        words_list = pickle.load(file_handler)\n",
    "\n",
    "        return word_vectors, words_list\n",
    "\n",
    "    def load_raw_data(self, raw_data_path):\n",
    "        raw_data = pd.read_csv(raw_data_path)\n",
    "        raw_data['original_text'] = raw_data['original_text'].apply(lambda x: self.convert_to_list(x))\n",
    "        self.raw_data = raw_data\n",
    "        return raw_data\n",
    "\n",
    "    @staticmethod\n",
    "    def convert_to_list(original_text_string):\n",
    "        original_text_list = ast.literal_eval(original_text_string)\n",
    "        original_text_list = [original_text.strip() for original_text in original_text_list]\n",
    "        return original_text_list\n",
    "\n",
    "    def generate_y(self, text, original_text_list):\n",
    "        y = np.zeros([self.max_sequence_length, 2])\n",
    "        try:\n",
    "            for i in range(len(y)):\n",
    "                y[i][0] = 1\n",
    "            for original_text_words in original_text_list:\n",
    "                for original_text in word_tokenize(original_text_words):\n",
    "                    words = word_tokenize(text)\n",
    "                    for i in range(len(words)):\n",
    "                        if words[i].lower() == original_text.lower():\n",
    "                            y[i][1] = 1\n",
    "                            y[i][0] = 0\n",
    "        except ValueError:\n",
    "            # print(ValueError)\n",
    "            pass\n",
    "        return y\n",
    "\n",
    "    def assign_y(self, raw_data):\n",
    "        y_list = np.zeros([len(raw_data), self.max_sequence_length, 2])\n",
    "        for body, original_text_list, i in zip(raw_data['body'], raw_data['original_text'], range(len(raw_data))):\n",
    "            try:\n",
    "                y_list[i] = self.generate_y(body, original_text_list)\n",
    "            except IndexError as e:\n",
    "                # print(e)\n",
    "                pass\n",
    "        return y_list\n",
    "\n",
    "    def generate_ids(self, raw_data, words_list, column_name):\n",
    "        #added column name\n",
    "        num_files = len(raw_data)\n",
    "        ids = np.zeros((num_files, self.max_sequence_length), dtype='int32')\n",
    "        file_counter = 0\n",
    "        for line in raw_data[column_name]:\n",
    "            split = word_tokenize(str(line))\n",
    "            index_counter = 0\n",
    "            for word in split:\n",
    "                try:\n",
    "                    ids[file_counter][index_counter] = words_list.index(word.lower())\n",
    "                except ValueError:\n",
    "                    ids[file_counter][index_counter] = len(words_list) - 2  # Vector for unkown words\n",
    "                index_counter = index_counter + 1\n",
    "                if index_counter >= self.max_sequence_length:\n",
    "                    break\n",
    "            file_counter = file_counter + 1\n",
    "\n",
    "        return ids\n",
    "\n",
    "    def get_character_set(self, text):\n",
    "        for char in text:\n",
    "            if char not in self.char_vocab:\n",
    "                self.char_vocab.append(char)\n",
    "\n",
    "    def generate_char_ids(self, raw_data, column_name):\n",
    "        # added column name\n",
    "        char_one_hot = np.zeros([len(self.char_vocab), len(self.char_vocab)])\n",
    "\n",
    "        for i in range(len(self.char_vocab)):\n",
    "            char_one_hot[i][i] = 1\n",
    "\n",
    "        ids_char = np.zeros([(len(raw_data)), self.max_sequence_length, self.max_char_length])\n",
    "        ids_char = ids_char + len(self.char_vocab) - 1\n",
    "\n",
    "        for i in range(len(raw_data)):\n",
    "            body = str(raw_data.iloc[i][column_name])\n",
    "            words = word_tokenize(body)\n",
    "            for j in (range(len(words))):\n",
    "                word = words[j]\n",
    "                for k in range(len(word)):\n",
    "                    char = word[k]\n",
    "                    try:\n",
    "                        ids_char[i][j][k] = self.char_vocab.index(char)\n",
    "                    except (IndexError, ValueError) as e:\n",
    "                        # print(e)\n",
    "                        pass\n",
    "\n",
    "        return char_one_hot, ids_char\n",
    "\n",
    "    def data_format(self, raw_data_path, word_vectors_path, words_list_path):\n",
    "        # Fetch word_vectors and wordslist\n",
    "        word_vectors, words_list = self.load_external_word_embeddings(self.word_vectors_path, self.words_list_path)\n",
    "\n",
    "        # Fetch raw_data\n",
    "        raw_data = self.load_raw_data(raw_data_path)\n",
    "\n",
    "        # Character pre_processing\n",
    "        raw_data['body'].apply(lambda x: self.get_character_set(str(x)))\n",
    "        raw_data['context'].apply(lambda x: self.get_character_set(str(x)))\n",
    "        # Obtain y\n",
    "        y_list = self.assign_y(raw_data)\n",
    "\n",
    "        # Generate ids for X\n",
    "        ids_body = self.generate_ids(raw_data=raw_data, words_list=words_list, column_name='body')\n",
    "        ids_context = self.generate_ids(raw_data=raw_data, words_list=words_list, column_name='context')\n",
    "\n",
    "        char_one_hot_body, ids_char_body = self.generate_char_ids(raw_data=raw_data, column_name='body')\n",
    "        char_one_hot_context, ids_char_context = self.generate_char_ids(raw_data=raw_data, column_name='context')\n",
    "\n",
    "        return word_vectors, words_list, ids_body, ids_context, y_list, char_one_hot_body, char_one_hot_context, ids_char_body, ids_char_context\n",
    "\n",
    "    def split_train_test(self, ids_body,ids_context,y_list, ids_char_body, ids_char_context):\n",
    "        # added body and context\n",
    "        # print('ids', ids)\n",
    "        X_train_body = ids_body[:self.train_len]\n",
    "        X_train_context = ids_context[:self.train_len]\n",
    "        y_train = y_list[:self.train_len]\n",
    "        X_test_body = ids_char_body[self.train_len:]\n",
    "        X_test_context = ids_char_context[self.train_len:]\n",
    "        y_test = y_list[self.train_len:]\n",
    "        ids_char_train_body = ids_char_body[:self.train_len]\n",
    "        ids_char_train_context = ids_char_context[:self.train_len]\n",
    "        ids_char_test_body = ids_char_body[self.train_len:]\n",
    "        ids_char_test_context = ids_char_context[self.train_len:]\n",
    "\n",
    "        # print('x_train', X_train)\n",
    "        return X_train_body, X_train_context, y_train, X_test_body, X_test_context,\\\n",
    "               y_test, ids_char_train_body, ids_char_train_context, ids_char_test_body, ids_char_test_context\n",
    "\n",
    "    def architecture(self, labels, input_data, char_input_data, train=True):\n",
    "\n",
    "        with tf.variable_scope(\"architecture\", reuse=tf.AUTO_REUSE):\n",
    "            n_nodes_hl1_body = 40\n",
    "            weights_body = {'W_conv1': tf.Variable(tf.random_normal([5, 5, 1, 32])),\n",
    "                       'W_conv2': tf.Variable(tf.random_normal([5, 5, 32, 64])),\n",
    "                       'W_fc': tf.Variable(tf.random_normal([(int(10 // 2)) * (int(4 // 2)) * 32, 100])),\n",
    "                       'out': tf.Variable(tf.random_normal([100, 10]))}\n",
    "\n",
    "            biases_body = {'b_conv1': tf.Variable(tf.random_normal([32])),\n",
    "                      'b_conv2': tf.Variable(tf.random_normal([64])),\n",
    "                      'b_fc': tf.Variable(tf.random_normal([100])),\n",
    "                      'out': tf.Variable(tf.random_normal([10]))}\n",
    "\n",
    "            hidden_1_layer_body = {\n",
    "                'weights': tf.Variable(tf.random_normal([1216, n_nodes_hl1_body])),\n",
    "                'biases': tf.Variable(tf.random_normal([n_nodes_hl1_body]))}\n",
    "\n",
    "            n_nodes_hl1_context = 40\n",
    "            weights_context = {'W_conv1': tf.Variable(tf.random_normal([5, 5, 1, 32])),\n",
    "                       'W_conv2': tf.Variable(tf.random_normal([5, 5, 32, 64])),\n",
    "                       'W_fc': tf.Variable(tf.random_normal([(int(10 // 2)) * (int(4 // 2)) * 32, 100])),\n",
    "                       'out': tf.Variable(tf.random_normal([100, 10]))}\n",
    "\n",
    "            biases_context = {'b_conv1': tf.Variable(tf.random_normal([32])),\n",
    "                      'b_conv2': tf.Variable(tf.random_normal([64])),\n",
    "                      'b_fc': tf.Variable(tf.random_normal([100])),\n",
    "                      'out': tf.Variable(tf.random_normal([10]))}\n",
    "\n",
    "            hidden_1_layer_context = {\n",
    "                'weights': tf.Variable(tf.random_normal([1216, n_nodes_hl1_body])),\n",
    "                'biases': tf.Variable(tf.random_normal([n_nodes_hl1_body]))}\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "            word_vectors, words_list, ids_body, ids_context, y_list, char_one_hot_body, char_one_hot_context, ids_char_body, ids_char_context = self.data_format(\n",
    "                raw_data_path=self.raw_data_path,\n",
    "                word_vectors_path=self.word_vectors_path,\n",
    "                words_list_path=self.words_list_path)\n",
    "\n",
    "            fw_cell3_context = tf.nn.rnn_cell.MultiRNNCell(\n",
    "                [tf.nn.rnn_cell.BasicLSTMCell(self.lstm_units, state_is_tuple=True, reuse=tf.AUTO_REUSE) for _ in\n",
    "                 range(self.lstm_layers)])\n",
    "            \n",
    "            bw_cell3_context = tf.nn.rnn_cell.MultiRNNCell(\n",
    "                [tf.nn.rnn_cell.BasicLSTMCell(self.lstm_units, state_is_tuple=True, reuse=tf.AUTO_REUSE) for _ in\n",
    "                 range(self.lstm_layers)])\n",
    "\n",
    "            fw_cell3_body = tf.nn.rnn_cell.MultiRNNCell(\n",
    "                [tf.nn.rnn_cell.BasicLSTMCell(self.lstm_units, state_is_tuple=True, reuse=tf.AUTO_REUSE) for _ in\n",
    "                 range(self.lstm_layers)])\n",
    "            bw_cell3_body = tf.nn.rnn_cell.MultiRNNCell(\n",
    "                [tf.nn.rnn_cell.BasicLSTMCell(self.lstm_units, state_is_tuple=True, reuse=tf.AUTO_REUSE) for _ in\n",
    "                 range(self.lstm_layers)])\n",
    "\n",
    "            # print('out ids', ids)\n",
    "\n",
    "            self.X_train_body, self.X_train_context, self.y_train, self.X_test_body, self.X_test_context, \\\n",
    "            self.y_test, self.ids_char_train_body, self.ids_char_train_context, self.ids_char_test_body, self.ids_char_test_context = self.split_train_test(ids_body,ids_context,y_list, ids_char_body,ids_char_context)\n",
    "\n",
    "            def character_level_embedding_body(char_input):\n",
    "                char_input = tf.cast(char_input, dtype=tf.int32)\n",
    "                char_one_hot_tensor = tf.convert_to_tensor(char_one_hot_body, dtype=tf.int32)\n",
    "                char_data = tf.nn.embedding_lookup(char_one_hot_tensor, char_input)\n",
    "                keep_rate = 1\n",
    "                x = tf.cast(\n",
    "                    tf.reshape(char_data,\n",
    "                               shape=[self.max_sequence_length, self.max_char_length * len(self.char_vocab)]),\n",
    "                    dtype=tf.float32)\n",
    "\n",
    "                def embedding_layer(data_fun):\n",
    "                    print(data_fun.shape)\n",
    "                    print(hidden_1_layer_body['weights'].shape)\n",
    "                    l1 = tf.add(tf.tensordot(data_fun, hidden_1_layer_body['weights'], axes=1), hidden_1_layer_body['biases'])\n",
    "                    l1 = tf.nn.relu(l1)\n",
    "                    # print(l1.shape)\n",
    "                    return l1\n",
    "\n",
    "                def conv2d(x_fun, w_fun):\n",
    "                    return tf.nn.conv2d(x_fun, w_fun, strides=[1, 1, 1, 1], padding='SAME')\n",
    "\n",
    "                def maxpool2d(x_fun):\n",
    "                    #                        size of window         movement of window\n",
    "                    return tf.nn.max_pool(x_fun, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME')\n",
    "\n",
    "                def convolutional_neural_network(x_fun):\n",
    "                    x_fun = tf.reshape(x_fun, shape=[self.max_sequence_length, 10, 4, 1])\n",
    "                    conv1 = tf.nn.relu(conv2d(x_fun, weights_body['W_conv1']) + biases_body['b_conv1'])\n",
    "                    conv2 = maxpool2d(conv1)\n",
    "                    fc = tf.reshape(conv2, [self.max_sequence_length, (10 // 2) * (4 // 2) * 32])\n",
    "                    fc = tf.nn.relu(tf.matmul(fc, weights_body['W_fc']) + biases_body['b_fc'])\n",
    "                    fc = tf.nn.dropout(fc, keep_rate)\n",
    "                    return fc\n",
    "\n",
    "                def train_neural_network(x_fun):\n",
    "                    print('x_fun', x_fun.shape)\n",
    "                    embedded_x = embedding_layer(x_fun)\n",
    "                    convoluted_x = convolutional_neural_network(embedded_x)\n",
    "                    return convoluted_x\n",
    "\n",
    "                char_data = train_neural_network(x)\n",
    "                return char_data\n",
    "\n",
    "            def character_level_embedding_context(char_input):\n",
    "                char_input = tf.cast(char_input, dtype=tf.int32)\n",
    "                char_one_hot_tensor = tf.convert_to_tensor(char_one_hot_body, dtype=tf.int32)\n",
    "                char_data = tf.nn.embedding_lookup(char_one_hot_tensor, char_input)\n",
    "                keep_rate = 1\n",
    "                x = tf.cast(\n",
    "                    tf.reshape(char_data,\n",
    "                               shape=[self.max_sequence_length, self.max_char_length * len(self.char_vocab)]),\n",
    "                    dtype=tf.float32)\n",
    "\n",
    "                def embedding_layer(data_fun):\n",
    "                    print(data_fun.shape)\n",
    "                    print(hidden_1_layer_body['weights'].shape)\n",
    "                    l1 = tf.add(tf.tensordot(data_fun, hidden_1_layer_body['weights'], axes=1), hidden_1_layer_body['biases'])\n",
    "                    l1 = tf.nn.relu(l1)\n",
    "                    # print(l1.shape)\n",
    "                    return l1\n",
    "\n",
    "                def conv2d(x_fun, w_fun):\n",
    "                    return tf.nn.conv2d(x_fun, w_fun, strides=[1, 1, 1, 1], padding='SAME')\n",
    "\n",
    "                def maxpool2d(x_fun):\n",
    "                    #                        size of window         movement of window\n",
    "                    return tf.nn.max_pool(x_fun, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME')\n",
    "\n",
    "                def convolutional_neural_network(x_fun):\n",
    "                    x_fun = tf.reshape(x_fun, shape=[self.max_sequence_length, 10, 4, 1])\n",
    "                    conv1 = tf.nn.relu(conv2d(x_fun, weights_body['W_conv1']) + biases_body['b_conv1'])\n",
    "                    conv2 = maxpool2d(conv1)\n",
    "                    fc = tf.reshape(conv2, [self.max_sequence_length, (10 // 2) * (4 // 2) * 32])\n",
    "                    fc = tf.nn.relu(tf.matmul(fc, weights_body['W_fc']) + biases_body['b_fc'])\n",
    "                    fc = tf.nn.dropout(fc, keep_rate)\n",
    "                    return fc\n",
    "\n",
    "                def train_neural_network(x_fun):\n",
    "                    print('x_fun', x_fun.shape)\n",
    "                    embedded_x = embedding_layer(x_fun)\n",
    "                    convoluted_x = convolutional_neural_network(embedded_x)\n",
    "                    return convoluted_x\n",
    "\n",
    "                char_data = train_neural_network(x)\n",
    "                return char_data\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "            def char_embeddings_layer_body(char_input):\n",
    "                print('ci', char_input.shape)\n",
    "                char_embeddings_fun = tf.map_fn(lambda x: character_level_embedding_body(x), char_input, dtype=tf.float32)\n",
    "                print('cef', char_embeddings_fun.shape)\n",
    "                return char_embeddings_fun\n",
    "            \n",
    "            def char_embeddings_layer_context(char_input):\n",
    "                print('ci', char_input.shape)\n",
    "                char_embeddings_fun = tf.map_fn(lambda x: character_level_embedding_body(x), char_input, dtype=tf.float32)\n",
    "                print('cef', char_embeddings_fun.shape)\n",
    "                return char_embeddings_fun\n",
    "\n",
    "            \n",
    "            def word_embeddings_layer_body(input_data_fun):\n",
    "                word_vectors_fun = tf.convert_to_tensor(word_vectors, dtype=tf.float32)\n",
    "                data_fun = tf.nn.embedding_lookup(word_vectors_fun, input_data_fun)\n",
    "                return data_fun\n",
    "\n",
    "            def word_embeddings_layer_context(input_data_fun):\n",
    "                word_vectors_fun = tf.convert_to_tensor(word_vectors, dtype=tf.float32)\n",
    "                data_fun = tf.nn.embedding_lookup(word_vectors_fun, input_data_fun)\n",
    "                return data_fun\n",
    "\n",
    "            \n",
    "            def char_word_embeddings_body(data_fun, char_embeddings_fun):\n",
    "                data2_fun = tf.concat([data_fun, char_embeddings_fun], axis=2)\n",
    "                return data2_fun\n",
    "            \n",
    "            def char_word_embeddings_context(data_fun, char_embeddings_fun):\n",
    "                data2_fun = tf.concat([data_fun, char_embeddings_fun], axis=2)\n",
    "                return data2_fun\n",
    "\n",
    "\n",
    "            def crf_layer_body(p2_fun, prediction_fun, sequence_lengths_fun, transition_params):\n",
    "\n",
    "                log_likelihood, transition_params_fun = tf.contrib.crf.crf_log_likelihood(\n",
    "                    inputs=prediction_fun,\n",
    "                    sequence_lengths=sequence_lengths_fun,\n",
    "                    tag_indices=p2_fun,\n",
    "                    transition_params=transition_params)\n",
    "                crf_loss = tf.reduce_mean(-log_likelihood)\n",
    "\n",
    "                return crf_loss, transition_params_fun\n",
    "\n",
    "            data_body = None\n",
    "            data_context = None\n",
    "            char_embeddings_body = None\n",
    "            char_embeddings_context = None\n",
    "            data2_body = None\n",
    "            data2_context = None\n",
    "            transition_params_temp_body = tf.Variable(tf.random_normal([self.num_classes, self.num_classes]))\n",
    "\n",
    "            if self.layer_parameters['word']:\n",
    "                data_body = word_embeddings_layer_body(input_data)\n",
    "                data2_body = data_body\n",
    "\n",
    "            if self.layer_parameters['char']:\n",
    "                char_embeddings_body = char_embeddings_layer_body(char_input_data)\n",
    "                data2_body = char_embeddings_body\n",
    "\n",
    "            if self.layer_parameters['word'] and self.layer_parameters['char']:\n",
    "                data2_body = char_word_embeddings_body(data_fun=data_body, char_embeddings_fun=char_embeddings_body)\n",
    "\n",
    "            outputs, value2 = tf.nn.bidirectional_dynamic_rnn(fw_cell3_body, bw_cell3_body, data2_body, dtype=tf.float32)\n",
    "\n",
    "            outputs = tf.concat(outputs, 2)\n",
    "\n",
    "            weight = tf.Variable(tf.truncated_normal([self.lstm_units * 2, self.num_classes]))\n",
    "            bias = tf.Variable(tf.constant(0.1, shape=[self.batch_size, self.max_sequence_length, self.num_classes]))\n",
    "\n",
    "            prediction = (tf.tensordot(outputs, weight, axes=((2,), (0,))) + bias)\n",
    "            p2 = tf.argmax(labels, axis=2)\n",
    "            prediction = tf.cast(prediction, dtype=tf.float32)\n",
    "            p2 = tf.cast(p2, dtype=tf.int32)\n",
    "\n",
    "\n",
    "            if self.layer_parameters['crf']:\n",
    "                sequence_lengths = tf.constant(shape=[self.batch_size],\n",
    "                                               value=np.zeros(self.batch_size) + self.max_sequence_length,\n",
    "                                               dtype=tf.int32)\n",
    "\n",
    "                loss, transition_params = crf_layer_body(sequence_lengths_fun=sequence_lengths, prediction_fun=prediction,\n",
    "                                                    p2_fun=p2, transition_params=transition_params_temp_body)\n",
    "\n",
    "            else:\n",
    "                loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=prediction, labels=labels))\n",
    "\n",
    "            optimizer = tf.train.AdamOptimizer().minimize(loss)\n",
    "\n",
    "            return optimizer, loss, weights_body, biases_body, hidden_1_layer_body, fw_cell3_body, bw_cell3_body, prediction, transition_params\n",
    "\n",
    "    def get_train_batch(self, j):\n",
    "        i = j * self.batch_size\n",
    "        arr_body = self.X_train_body[i:i + self.batch_size]\n",
    "        arr_context = self.X_train_context[i:i + self.batch_size]\n",
    "        labels = self.y_train[i:i + self.batch_size]\n",
    "        arr_char_body = self.ids_char_train_body[i:i + self.batch_size]\n",
    "        arr_char_context = self.ids_char_train_context[i:i + self.batch_size]\n",
    "        return arr_body, arr_context, labels, arr_char_body, arr_char_context\n",
    "\n",
    "    def get_test_batch(self, j):\n",
    "        i = j * self.batch_size\n",
    "        arr_body = self.X_test_body[i:i + self.batch_size]\n",
    "        arr_context = self.X_test_context[i:i + self.batch_size]\n",
    "        labels = self.y_test[i:i + self.batch_size]\n",
    "        arr_char_body = self.ids_char_test_body[i:i + self.batch_size]\n",
    "        arr_char_context = self.ids_char_test_body[i:i + self.batch_size]\n",
    "        return arr_body, arr_context, labels, arr_char_body, arr_char_context\n",
    "\n",
    "    def train_architecture(self):\n",
    "\n",
    "        labels = tf.placeholder(tf.float32, [self.batch_size, self.max_sequence_length, self.num_classes])\n",
    "        input_data = tf.placeholder(tf.int32, [self.batch_size, self.max_sequence_length])\n",
    "        char_input_data = tf.placeholder(dtype=tf.float32,\n",
    "                                         shape=[self.batch_size, self.max_sequence_length, self.max_char_length])\n",
    "\n",
    "        optimizer, loss, weights, biases, hidden_1_layer, fw_cell3, bw_cell3, prediction, transition_params = \\\n",
    "            self.architecture(\n",
    "                labels, input_data, char_input_data)\n",
    "\n",
    "        init_g = tf.global_variables_initializer()\n",
    "        init_l = tf.local_variables_initializer()\n",
    "        init = tf.initialize_all_variables()\n",
    "\n",
    "        with tf.Session() as sess:\n",
    "            sess.run(init_g)\n",
    "            sess.run(init_l)\n",
    "            sess.run(init)\n",
    "            saver = tf.train.Saver()\n",
    "            for i in range(self.epochs):\n",
    "                print(\"epoch:  \" + str(i))\n",
    "                losses = 0\n",
    "                for j in range(self.train_len // self.batch_size):\n",
    "                    next_batch_body, next_batch_context, next_batch_labels, next_chars_body, next_chars_context = self.get_train_batch(j)\n",
    "                    _, ll = sess.run([optimizer, loss],\n",
    "                                     {input_data: next_batch_body, labels: next_batch_labels, char_input_data: next_chars_body})\n",
    "                    losses += ll\n",
    "                save_path = saver.save(sess, self.save_model)\n",
    "                print(\"saved to %s\" % save_path)\n",
    "                print(losses)\n",
    "\n",
    "    def train_test_architecture(self):\n",
    "\n",
    "        labels = tf.placeholder(tf.float32, [self.batch_size, self.max_sequence_length, self.num_classes])\n",
    "        input_data = tf.placeholder(tf.int32, [self.batch_size, self.max_sequence_length])\n",
    "        char_input_data = tf.placeholder(dtype=tf.float32,\n",
    "                                         shape=[self.batch_size, self.max_sequence_length, self.max_char_length])\n",
    "\n",
    "        optimizer, loss, weights, biases, hidden_1_layer, fw_cell3, bw_cell3, prediction, transition_params = \\\n",
    "            self.architecture(\n",
    "                labels, input_data, char_input_data)\n",
    "\n",
    "        init_g = tf.global_variables_initializer()\n",
    "        init_l = tf.local_variables_initializer()\n",
    "        init = tf.initialize_all_variables()\n",
    "\n",
    "        fin_pred = np.zeros([self.test_batches * self.batch_size, self.max_sequence_length, 1])\n",
    "\n",
    "        with tf.Session() as sess:\n",
    "            sess.run(init_g)\n",
    "            sess.run(init_l)\n",
    "            sess.run(init)\n",
    "            saver = tf.train.Saver()\n",
    "            for i in range(self.epochs):\n",
    "                print(\"epoch:  \" + str(i))\n",
    "                losses = 0\n",
    "                for j in range(self.train_len // self.batch_size):\n",
    "                    next_batch_body, next_batch_context, next_batch_labels, next_chars_body, next_chars_context = self.get_train_batch(j)\n",
    "                    _, ll = sess.run([optimizer, loss],\n",
    "                                     {input_data: next_batch_body, labels: next_batch_labels, char_input_data: next_chars_body})\n",
    "                    losses += ll\n",
    "                save_path = saver.save(sess, self.save_model)\n",
    "                print(\"saved to %s\" % save_path)\n",
    "                print(losses)\n",
    "\n",
    "            for j in range(self.test_batches):\n",
    "                jj = j * self.batch_size\n",
    "                next_batch_body, next_batch_context, next_batch_labels, next_chars_body, next_chars_context = self.get_test_batch(j)\n",
    "                p, t = sess.run([prediction, transition_params],\n",
    "                                {input_data: next_batch_body, labels: next_batch_labels, char_input_data: next_chars_body})\n",
    "\n",
    "                for k in range(self.batch_size):\n",
    "                    viterbi_sequence, viterbi_score = tf.contrib.crf.viterbi_decode(p[k], t)\n",
    "                    viterbi_sequence = np.array(viterbi_sequence).reshape(self.max_sequence_length, 1)\n",
    "                    fin_pred[jj:jj + 100][k] = viterbi_sequence\n",
    "\n",
    "        prediction_analysis = {'expected': [], 'predicted': [], 'body': []}\n",
    "\n",
    "        for i in range((self.test_batches * self.batch_size)):\n",
    "            body = word_tokenize(str(self.raw_data[self.train_len:].iloc[i]['body']))\n",
    "            lab_index_test = np.where(np.argmax(self.y_test, axis=2)[i] == 1)[0]\n",
    "            lab_index_pred = np.where(fin_pred[i] == 1)[0]\n",
    "            original_text = []\n",
    "            pred_original_text = []\n",
    "            for j in lab_index_test:\n",
    "                original_text.append(body[j])\n",
    "            for j in lab_index_pred:\n",
    "                try:\n",
    "                    pred_original_text.append(body[j])\n",
    "                except IndexError:\n",
    "                    pass\n",
    "            prediction_analysis['expected'].append(original_text)\n",
    "            prediction_analysis['predicted'].append(pred_original_text)\n",
    "            prediction_analysis['body'].append(body)\n",
    "        prediction_analysis_pandas = pd.DataFrame.from_dict(prediction_analysis)\n",
    "\n",
    "        return prediction_analysis_pandas\n",
    "\n",
    "    def test_architecture(self):\n",
    "        labels = tf.placeholder(tf.float32, [self.batch_size, self.max_sequence_length, self.num_classes])\n",
    "        input_data = tf.placeholder(tf.int32, [self.batch_size, self.max_sequence_length])\n",
    "        char_input_data = tf.placeholder(dtype=tf.float32,\n",
    "                                         shape=[self.batch_size, self.max_sequence_length, self.max_char_length])\n",
    "\n",
    "        fin_pred = np.zeros([self.test_batches * self.batch_size, self.max_sequence_length, 1])\n",
    "\n",
    "        optimizer, loss, weights, biases, hidden_1_layer, fw_cell3, bw_cell3, prediction, transition_params = \\\n",
    "            self.architecture(\n",
    "                labels, input_data, char_input_data, train=False)\n",
    "\n",
    "        saver = tf.train.Saver()\n",
    "        # tf.reset_default_graph()\n",
    "        # init_g = tf.global_variables_initializer()\n",
    "        # init_l = tf.local_variables_initializer()\n",
    "        # init = tf.initialize_all_variables()\n",
    "\n",
    "        with tf.Session() as sess:\n",
    "            # sess.run(init_g)\n",
    "            # sess.run(init_l)\n",
    "            # sess.run(init)\n",
    "\n",
    "            saver.restore(sess, self.save_model)\n",
    "            for j in range(self.test_batches):\n",
    "                jj = j * self.batch_size\n",
    "                next_batch, next_batch_labels, next_chars = self.get_test_batch(j)\n",
    "                p, t = sess.run([prediction, transition_params],\n",
    "                                {input_data: next_batch, labels: next_batch_labels, char_input_data: next_chars})\n",
    "\n",
    "                for k in range(self.batch_size):\n",
    "                    viterbi_sequence, viterbi_score = tf.contrib.crf.viterbi_decode(p[k], t)\n",
    "                    viterbi_sequence = np.array(viterbi_sequence).reshape(self.max_sequence_length, 1)\n",
    "                    fin_pred[jj:jj + 100][k] = viterbi_sequence\n",
    "\n",
    "        return fin_pred\n",
    "\n",
    "    def prediction_pandas(self):\n",
    "        fin_pred = self.test_architecture()\n",
    "        prediction_analysis = {'expected': [], 'predicted': [], 'body': []}\n",
    "        for i in range((self.test_batches * self.batch_size)):\n",
    "\n",
    "            body = word_tokenize(str(self.raw_data[self.train_len:].iloc[i]['body']))\n",
    "            lab_index_test = np.where(np.argmax(self.y_test, axis=2)[i] == 1)[0]\n",
    "            lab_index_pred = np.where(fin_pred[i] == 1)[0]\n",
    "            original_text = []\n",
    "            pred_original_text = []\n",
    "            for j in lab_index_test:\n",
    "                original_text.append(body[j])\n",
    "            for j in lab_index_pred:\n",
    "                try:\n",
    "                    pred_original_text.append(body[j])\n",
    "                except IndexError:\n",
    "                    pass\n",
    "            prediction_analysis['expected'].append(original_text)\n",
    "            prediction_analysis['predicted'].append(pred_original_text)\n",
    "            prediction_analysis['body'].append(body)\n",
    "        prediction_analysis_pandas = pd.DataFrame.from_dict(prediction_analysis)\n",
    "        return prediction_analysis_pandas\n",
    "\n",
    "    def get_prediction_analysis(prediction_analysis):\n",
    "\n",
    "        #         prediction_analysis = self.get_predictions(data)\n",
    "        tp = []\n",
    "        fp = []\n",
    "        fn = []\n",
    "        for predicted, expected in zip(prediction_analysis['predicted'], prediction_analysis['expected']):\n",
    "            predicted = set([x.lower() for x in predicted])\n",
    "            expected = set([y.lower() for x in expected for y in word_tokenize(x)])\n",
    "\n",
    "            tp.append(predicted.intersection(expected))\n",
    "            fp.append(predicted - predicted.intersection(expected))\n",
    "            fn.append(expected - predicted.intersection(expected))\n",
    "\n",
    "        prediction_analysis['tp'] = list(tp)\n",
    "        prediction_analysis['fp'] = list(fp)\n",
    "        prediction_analysis['fn'] = list(fn)\n",
    "\n",
    "        return prediction_analysis\n",
    "        # pp = tru_pos / (tru_pos + fals_pos)\n",
    "        # rr = tru_pos / (tru_pos + fals_neg)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training\n",
    " This is where you create an instance of the class.\n",
    " \n",
    " And run and obtain the test results using train_test_architecture\n",
    " \n",
    " get_pred_analysis is used to get the result in a proper format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/pratik/anaconda3/lib/python3.6/site-packages/tensorflow/python/util/tf_should_use.py:118: initialize_all_variables (from tensorflow.python.ops.variables) is deprecated and will be removed after 2017-03-02.\n",
      "Instructions for updating:\n",
      "Use `tf.global_variables_initializer` instead.\n",
      "epoch:  0\n",
      "saved to models/malubu_lstm_context.ckpt\n",
      "333.5782597064972\n",
      "epoch:  1\n",
      "saved to models/malubu_lstm_context.ckpt\n",
      "175.85130321979523\n",
      "epoch:  2\n",
      "saved to models/malubu_lstm_context.ckpt\n",
      "132.21516877412796\n",
      "epoch:  3\n",
      "saved to models/malubu_lstm_context.ckpt\n",
      "95.1178429722786\n",
      "epoch:  4\n",
      "saved to models/malubu_lstm_context.ckpt\n",
      "79.16474491357803\n"
     ]
    }
   ],
   "source": [
    "\n",
    "model = CrfLstmModel( max_sequence_length=MAX_SEQUENCE_LENGTH, \n",
    "                     max_char_length=MAX_CHAR_LENGTH,\n",
    "                     lstm_units=LSTM_UNITS,\n",
    "                     num_classes=NUM_CLASSES, \n",
    "                     epochs=EPOCHS,\n",
    "                     num_dimensions=NUM_DIMENSIONS,\n",
    "                     batch_size=BATCH_SIZE,\n",
    "                     train_len=TRAIN_LENGTH,\n",
    "                     lstm_layers=LSTM_LAYERS,\n",
    "                     layer_parameters=LAYER_PARAMETERS,\n",
    "                 test_batches=TEST_BATCHES,\n",
    "                    save_model=SAVE_MODEL\n",
    "                     , raw_data_path=RAW_DATA_PATH,\n",
    "                     word_vectors_path=WORD_VECTORS_PATH,\n",
    "                     words_list_path=WORDS_LIST_PATH)\n",
    "# word_vectors, words_list, ids_body, ids_context, y_list, char_one_hot_body, char_one_hot_context, ids_char_body, ids_char_context = model.data_format(\n",
    "#                 raw_data_path=RAW_DATA_PATH,\n",
    "#                 word_vectors_path=WORD_VECTORS_PATH,\n",
    "#                 words_list_path=WORDS_LIST_PATH)\n",
    "\n",
    "model.train_architecture()\n",
    "#jj2 = model.train_test_architecture()\n",
    "# jj = CrfLstmModel.get_prediction_analysis(jj2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.X_train_body, model.X_train_context, model.y_train, model.X_test_body, model.X_test_context, \\\n",
    "# model.y_test, model.ids_char_train_body, model.ids_char_train_context, model.ids_char_test_body, model.ids_char_test_context = model.split_train_test(ids_body,ids_context,y_list, ids_char_body, ids_char_context)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "word_vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model.test_architecture()\n",
    "#model.prediction_pandas()\n",
    "#model = CrfLstmModel(layer_parameters=layer_parameters)\n",
    "#model.prediction_pandas_analysis()\n",
    "#model.prediction_pandas_analysis()\n",
    "#jj2 = model.prediction_pandas()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**fp_analysis** consists of the false positive examples\n",
    "\n",
    "**fn_analysis**  consists of the false negative examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'jj' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-a39ba35004ed>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mfp_analysis\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjj\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjj\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'fp'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m!=\u001b[0m\u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mfn_analysis\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjj\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjj\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'fn'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m!=\u001b[0m\u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'jj' is not defined"
     ]
    }
   ],
   "source": [
    "fp_analysis = jj[(jj['fp']!=set())]\n",
    "fn_analysis = jj[(jj['fn']!=set())]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some PreProcessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_plot_test = model.raw_data[model.train_len:]\n",
    "def separate(original_text_list):\n",
    "    return [each for original_text in original_text_list for each in word_tokenize(original_text) ]\n",
    "\n",
    "X_plot_test['original_text'] = X_plot_test['original_text'].apply(lambda x: separate(x))  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Welcome to result Visualisation\n",
    "Kindly do not try to dig deep into the code as it is not optimized and a little messy **:(** . \n",
    "\n",
    "Take insights from the graphs plotted for you. An explaination for each plot is provided.\n",
    "\n",
    "For good visualization just mute out the code blocks\n",
    "\n",
    "**Each plot consists of three graphs illustrating the False Positives, False Negatives, Test Data in that respective order"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PLOT 1\n",
    "#### This plot gives us confusion matrix distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count = 0\n",
    "def get_count(original_text_list):\n",
    "    global count\n",
    "    original_text_list = list(original_text_list)\n",
    "    count += len(original_text_list)\n",
    "\n",
    "jj['tp'].apply(lambda x: get_count(x))\n",
    "tp = count\n",
    "count = 0\n",
    "jj['fp'].apply(lambda x: get_count(x))\n",
    "fp = count\n",
    "count = 0\n",
    "jj['fn'].apply(lambda x: get_count(x))\n",
    "fn = count\n",
    "count=0\n",
    "jj['expected'].apply(lambda x: get_count(x))\n",
    "pos = count\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10,4))\n",
    "x = ['TP', 'FP', 'FN', 'POSITIVES']\n",
    "y = [tp, fp, fn, pos]\n",
    "print(tp,fp,fn,pos)\n",
    "plt.subplot(121)\n",
    "sns.barplot(x=x, y=y)\n",
    "\n",
    "x = ['PRECISION', 'RECALL', 'F_SCORE']\n",
    "prec = float(tp)/(tp+fp)\n",
    "rec = float(tp)/(tp+fn)\n",
    "\n",
    "f_score = (2 * prec * rec)/ (prec + rec)\n",
    "y = [prec, rec, f_score]\n",
    "plt.subplot(122)\n",
    "sns.barplot(x=x, y=y)\n",
    "print(f_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PLOT 2\n",
    "This plot gives us the number of entities found in each sentence.\n",
    "\n",
    "This plot gives us a rough sense of how well the classifier is doing on samples with different number of entities.\n",
    "\n",
    "How well is it performing on samples with one, two, two+ entities.\n",
    "\n",
    "Find me the nearest railway station. (Entity count 2, [\"railway\", \"station\"])\n",
    "\n",
    "Where is the nearest atm (Entitys count 1, [\"atm\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = [0, 0, 0, 0]\n",
    "y = ['zero', 'one', 'two', 'two+']\n",
    "def get_sample_count(original_text_list):\n",
    "    global x\n",
    "    count = 0\n",
    "    for original_text in original_text_list:\n",
    "        count += len(word_tokenize(original_text))\n",
    "    if count < len(x):\n",
    "        x[count] += 1\n",
    "    else:\n",
    "        x[-1] += 1\n",
    "\n",
    "        \n",
    "        \n",
    "fig, ax = plt.subplots(figsize=(10,4))\n",
    "\n",
    "\n",
    "\n",
    "fp_analysis['fp'].apply(lambda x: get_sample_count(list(x)))\n",
    "np.array(x).sum()\n",
    "sns.set_style(\"whitegrid\")\n",
    "plt.subplot(131)\n",
    "sns.barplot(x=y, y=x)\n",
    "plt.title('FALSE POSITIVES')\n",
    "\n",
    "x = [0, 0, 0, 0]\n",
    "y = ['zero', 'one', 'two', 'two+']\n",
    "fn_analysis['fn'].apply(lambda x: get_sample_count(list(x)))\n",
    "np.array(x).sum()\n",
    "plt.subplot(132)\n",
    "sns.barplot(x=y, y=x)\n",
    "plt.title('FALSE NEGATIVES')\n",
    "\n",
    "\n",
    "x = [0, 0, 0, 0]\n",
    "y = ['zero', 'one', 'two', 'two+']\n",
    "X_plot_test['original_text'].apply(lambda x: get_sample_count(list(x)))\n",
    "np.array(x).sum()\n",
    "plt.subplot(133)\n",
    "sns.barplot(x=y, y=x)\n",
    "plt.title('TEST DATA')\n",
    "# Show the plot                   \n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PLOT 3\n",
    "This plot gives us the each specific entity occurences\n",
    "\n",
    "This plot gives us a rough sense of how well the classifier is doing on specific entities.\n",
    "\n",
    "We can pin point to exact entities if they are consistently getting miscalssified\n",
    "In the example below Graph 3. There are approximately 60 atm occurences\n",
    "\n",
    "**Top 7 entities **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_dict = {}\n",
    "def get_entities_count(original_text_list):\n",
    "    global x_dict\n",
    "    for original_text in original_text_list:\n",
    "        original_text_lower = original_text.lower()\n",
    "        if original_text_lower not in x_dict.keys():\n",
    "            x_dict[original_text_lower] = 0\n",
    "        x_dict[original_text_lower] += 1\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10,8))\n",
    "\n",
    "fig.tight_layout()\n",
    "plt.subplots_adjust(hspace=.5)\n",
    "fp_analysis['fp'].apply(lambda x: get_entities_count(list(x)))\n",
    "unique_entity_list = pd.DataFrame({'entity': list(x_dict.keys()), 'count': list(x_dict.values())})\n",
    "unique_entity_list.sort_values(by='count', ascending=[0], inplace=True)\n",
    "plt.subplot(311)\n",
    "sns.barplot(x='entity', y='count', data=unique_entity_list.head(7))\n",
    "plt.title('FALSE POSITIVES')\n",
    "\n",
    "x_dict = {}\n",
    "fn_analysis['fn'].apply(lambda x: get_entities_count(list(x)))\n",
    "unique_entity_list = pd.DataFrame({'entity': list(x_dict.keys()), 'count': list(x_dict.values())})\n",
    "unique_entity_list.sort_values(by=['count'], ascending=[0], inplace=True)\n",
    "plt.subplot(312)\n",
    "sns.barplot(x='entity', y='count', data=unique_entity_list.head(7))\n",
    "plt.title('FALSE NEGATIVES')\n",
    "\n",
    "x_dict = {}\n",
    "X_plot_test['original_text'].apply(lambda x: get_entities_count(list(x)))\n",
    "unique_entity_list = pd.DataFrame({'entity': list(x_dict.keys()), 'count': list(x_dict.values())})\n",
    "unique_entity_list.sort_values(by=['count'], ascending=[0], inplace=True)\n",
    "plt.subplot(313)\n",
    "sns.barplot(x='entity', y='count', data=unique_entity_list.head(7))\n",
    "plt.title('TEST DATA')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PLOT 4\n",
    "This plot gives us the each specific word occuring before and after each entity.\n",
    "\n",
    "This helps us understand the words before and after or inshort the patterns that occur surrounding the entity\n",
    "\n",
    "We can understand the patterns which are consistently misclassified\n",
    "START: The entity is at the start of the sentence hence no words before\n",
    "atm near me\n",
    "\n",
    "END: The entity is at the end of the sentence hence no words after\n",
    "nearest atm\n",
    "\n",
    "ONLY: The sentence only consists of entity alone\n",
    "pizza hut\n",
    "**Top 7 entities depicted**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "before_dict = {}\n",
    "after_dict = {}\n",
    "before_dict['only'] = 0\n",
    "after_dict['only'] = 0\n",
    "def get_words_before(text, original_text_list):\n",
    "    text = ' '.join(text)\n",
    "    global before_dict, after_dict, only\n",
    "    only = end_status = start_status = False\n",
    "    text = text.lower()\n",
    "    for original_text in original_text_list:\n",
    "        original_text = original_text.lower()\n",
    "        regex = re.compile(r'(\\w*[,|:|?]*)\\s*%s\\s*(\\w*)' %(original_text))\n",
    "        before = after = None\n",
    "        try:\n",
    "            before, after = regex.findall(text)[0]\n",
    "        except:\n",
    "            print(text)\n",
    "        if not (before or after):\n",
    "            before_dict['only'] += 1\n",
    "            after_dict['only'] += 1\n",
    "            only = True\n",
    "            \n",
    "        before = before or 'START'\n",
    "        after = after or 'END'\n",
    "        \n",
    "        if before == 'START':\n",
    "            start_status = True\n",
    "        if after == 'END':\n",
    "            end_status = True\n",
    "        \n",
    "        \n",
    "        if before not in before_dict:\n",
    "            before_dict[before] = 0\n",
    "        if after not in after_dict:\n",
    "            after_dict[after] = 0\n",
    "        \n",
    "        before_dict[before] += 1\n",
    "        after_dict[after] += 1\n",
    "    return {'s':start_status, 'e': end_status}\n",
    "fig, ax = plt.subplots(figsize=(10,8))\n",
    "fig.tight_layout()\n",
    "plt.subplots_adjust(hspace=.5)\n",
    "fp_analysis['start_end'] = fp_analysis.apply(lambda x: get_words_before(x['body'], x['expected']), axis=1)       \n",
    "fp_analysis['start'] = fp_analysis['start_end'].apply(lambda x: x['s'])\n",
    "fp_analysis['end'] = fp_analysis['start_end'].apply(lambda x: x['e'])\n",
    "fp_analysis['only'] = fp_analysis.apply(lambda x: x['start'] and x['end'], axis=1)\n",
    "fp_analysis.drop('start_end', inplace=True, axis=1)\n",
    "before_pattern_list = pd.DataFrame({'before': list(before_dict.keys()), 'count': list(before_dict.values())})\n",
    "before_pattern_list.sort_values(by=['count'], ascending=[0], inplace=True)\n",
    "after_pattern_list = pd.DataFrame({'after': list(after_dict.keys()), 'count':list( after_dict.values())})\n",
    "after_pattern_list.sort_values(by=['count'], ascending=[0], inplace=True)\n",
    "plt.subplot(321)\n",
    "sns.barplot(x='before', y='count', data=before_pattern_list.head(10))\n",
    "plt.title('FALSE POSITIVES BEFORE')\n",
    "plt.subplot(322)\n",
    "sns.barplot(x='after', y='count', data=after_pattern_list.head(10))\n",
    "plt.title('FALSE POSITIVES AFTER')\n",
    "\n",
    "\n",
    "before_dict = {}\n",
    "after_dict = {}\n",
    "before_dict['only'] = 0\n",
    "after_dict['only'] = 0\n",
    "\n",
    "\n",
    "fn_analysis['start_end'] = fn_analysis.apply(lambda x: get_words_before(x['body'], x['expected']), axis=1)       \n",
    "fn_analysis['start'] = fn_analysis['start_end'].apply(lambda x: x['s'])\n",
    "fn_analysis['end'] = fn_analysis['start_end'].apply(lambda x: x['e'])\n",
    "fn_analysis['only'] = fn_analysis.apply(lambda x: x['start'] and x['end'], axis=1)\n",
    "fn_analysis.drop('start_end', inplace=True, axis=1)\n",
    "before_pattern_list = pd.DataFrame({'before': list(before_dict.keys()), 'count': list(before_dict.values())})\n",
    "before_pattern_list.sort_values(by=['count'], ascending=[0], inplace=True)\n",
    "after_pattern_list = pd.DataFrame({'after': list(after_dict.keys()), 'count': list(after_dict.values())})\n",
    "after_pattern_list.sort_values(by=['count'], ascending=[0], inplace=True)\n",
    "plt.subplot(323)\n",
    "sns.barplot(x='before', y='count', data=before_pattern_list.head(10))\n",
    "plt.title('FALSE NEGATIVES BEFORE')\n",
    "plt.subplot(324)\n",
    "sns.barplot(x='after', y='count', data=after_pattern_list.head(10))\n",
    "plt.title('FALSE NEGATIVES AFTER')\n",
    "\n",
    "before_dict = {}\n",
    "after_dict = {}\n",
    "before_dict['only'] = 0\n",
    "after_dict['only'] = 0\n",
    "\n",
    "\n",
    "X_plot_test['start_end'] = X_plot_test.apply(lambda x: get_words_before([p for p in word_tokenize(x['body'])], x['original_text']), axis=1)       \n",
    "X_plot_test['start'] = X_plot_test['start_end'].apply(lambda x: x['s'])\n",
    "X_plot_test['end'] = X_plot_test['start_end'].apply(lambda x: x['e'])\n",
    "X_plot_test['only'] = X_plot_test.apply(lambda x: x['start'] and x['end'], axis=1)\n",
    "X_plot_test.drop('start_end', inplace=True, axis=1)\n",
    "\n",
    "before_pattern_list = pd.DataFrame({'before': list(before_dict.keys()), 'count': list(before_dict.values())})\n",
    "before_pattern_list.sort_values(by=['count'], ascending=[0], inplace=True)\n",
    "after_pattern_list = pd.DataFrame({'after': list(after_dict.keys()), 'count': list(after_dict.values())})\n",
    "after_pattern_list.sort_values(by=['count'], ascending=[0], inplace=True)\n",
    "plt.subplot(325)\n",
    "sns.barplot(x='before', y='count', data=before_pattern_list.head(10))\n",
    "plt.title('TEST DATA BEFORE')\n",
    "\n",
    "plt.subplot(326)\n",
    "sns.barplot(x='after', y='count', data=after_pattern_list.head(10))\n",
    "plt.title('TEST DATA AFTER')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PLOT 5\n",
    "This plot gives us the length distribution of the samples\n",
    "\n",
    "This gives an overview of the model's performance on samples of different lengths\n",
    "\n",
    "**The length considered here is the number of tokens in a sentence**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(10,5))\n",
    "\n",
    "print('FALSE POSITIVE MEDIAN', fp_analysis['body'].apply(lambda x: len(x)).median())\n",
    "plt.subplot(1,3,1)\n",
    "sns.distplot(fp_analysis['body'].apply(lambda x: len(x)), kde=False, rug=True)\n",
    "plt.title('FALSE POSITIVES')\n",
    "\n",
    "plt.subplot(1,3,2)\n",
    "print('FALSE NEGATIVE MEDIAN', fn_analysis['body'].apply(lambda x: len(x)).median())\n",
    "sns.distplot(fn_analysis['body'].apply(lambda x: len((x))), kde=False, rug=True)\n",
    "plt.title('FALSE NEGATIVES')\n",
    "\n",
    "\n",
    "plt.subplot(1,3,3)\n",
    "print('TEST DATA MEDIAN', X_plot_test['body'].apply(lambda x: len(word_tokenize(x))).median())\n",
    "sns.distplot(X_plot_test['body'].apply(lambda x: len(word_tokenize(x))), kde=False, rug=True)\n",
    "plt.title('TEST DATA')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_count(original_text_list):\n",
    "    global count\n",
    "    original_text_list = list(original_text_list)\n",
    "    count += len(original_text_list)\n",
    "\n",
    "train_size_analysis = {'train':[], 'train_size': [], 'precision': [], 'recall': [], 'f_score': [] }\n",
    "train_sizes = [10, 20, 30, 50, 70, 80, 100, 200, 300, 500,600, 800, 1000,1200 , 1500,1700,2000,2200, 2500]\n",
    "f_score_sizes = []\n",
    "for train_size in train_sizes:\n",
    "    count = 0\n",
    "    print('Running for Train size: ',train_size )\n",
    "    model = CrfLstmModel( max_sequence_length=MAX_SEQUENCE_LENGTH, \n",
    "                         max_char_length=MAX_CHAR_LENGTH,\n",
    "                         lstm_units=LSTM_UNITS,\n",
    "                         num_classes=NUM_CLASSES, \n",
    "                         epochs=EPOCHS,\n",
    "                         num_dimensions=NUM_DIMENSIONS,\n",
    "                         batch_size=BATCH_SIZE,\n",
    "                         train_len=train_size,\n",
    "                         lstm_layers=LSTM_LAYERS,\n",
    "                         layer_parameters=LAYER_PARAMETERS,\n",
    "                     test_batches=TEST_BATCHES,\n",
    "                        save_model=SAVE_MODEL\n",
    "                         , raw_data_path=RAW_DATA_PATH,\n",
    "                         word_vectors_path=WORD_VECTORS_PATH,\n",
    "                         words_list_path=WORDS_LIST_PATH)\n",
    "\n",
    "    jj2 = model.train_test_architecture()\n",
    "    jj = CrfLstmModel.get_prediction_analysis(jj2)\n",
    "    jj['tp'].apply(lambda x: get_count(x))\n",
    "    tp = count\n",
    "    count = 0\n",
    "    jj['fp'].apply(lambda x: get_count(x))\n",
    "    fp = count\n",
    "    count = 0\n",
    "    jj['fn'].apply(lambda x: get_count(x))\n",
    "    fn = count\n",
    "    count=0\n",
    "    jj['expected'].apply(lambda x: get_count(x))\n",
    "    pos = count\n",
    "    print(tp, fp, fn, pos)\n",
    "    #print(jj)\n",
    "    print('---------------------------------------------------------------------------------')\n",
    "    f_score = 0\n",
    "    try:\n",
    "        prec = float(tp)/(tp+fp)\n",
    "        rec = float(tp)/(tp+fn)\n",
    "        f_score = (2 * prec * rec)/ (prec + rec)\n",
    "    except:\n",
    "        pass\n",
    "        \n",
    "    f_score_sizes.append(f_score)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(10,4))\n",
    "x = train_sizes\n",
    "y = f_score_sizes\n",
    "\n",
    "sns.barplot(x=x, y=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_count(original_text_list):\n",
    "    global count\n",
    "    original_text_list = list(original_text_list)\n",
    "    count += len(original_text_list)\n",
    "\n",
    "\n",
    "epoch_sizes = [5,10,15,20,25]\n",
    "f_score_sizes = []\n",
    "for epoch_size in epoch_sizes:\n",
    "    count = 0\n",
    "    print('Running for Epoch size: ',epoch_size )\n",
    "    model = CrfLstmModel( max_sequence_length=MAX_SEQUENCE_LENGTH, \n",
    "                         max_char_length=MAX_CHAR_LENGTH,\n",
    "                         lstm_units=LSTM_UNITS,\n",
    "                         num_classes=NUM_CLASSES, \n",
    "                         epochs=epoch_size,\n",
    "                         num_dimensions=NUM_DIMENSIONS,\n",
    "                         batch_size=BATCH_SIZE,\n",
    "                         train_len=TRAIN_LENGTH,\n",
    "                         lstm_layers=LSTM_LAYERS,\n",
    "                         layer_parameters=LAYER_PARAMETERS,\n",
    "                     test_batches=TEST_BATCHES,\n",
    "                        save_model=SAVE_MODEL\n",
    "                         , raw_data_path=RAW_DATA_PATH,\n",
    "                         word_vectors_path=WORD_VECTORS_PATH,\n",
    "                         words_list_path=WORDS_LIST_PATH)\n",
    "\n",
    "    jj2 = model.train_test_architecture()\n",
    "    jj = CrfLstmModel.get_prediction_analysis(jj2)\n",
    "    jj['tp'].apply(lambda x: get_count(x))\n",
    "    tp = count\n",
    "    count = 0\n",
    "    jj['fp'].apply(lambda x: get_count(x))\n",
    "    fp = count\n",
    "    count = 0\n",
    "    jj['fn'].apply(lambda x: get_count(x))\n",
    "    fn = count\n",
    "    count=0\n",
    "    jj['expected'].apply(lambda x: get_count(x))\n",
    "    pos = count\n",
    "    print(tp, fp, fn, pos)\n",
    "    #print(jj)\n",
    "    print('---------------------------------------------------------------------------------')\n",
    "    f_score = 0\n",
    "    try:\n",
    "        prec = float(tp)/(tp+fp)\n",
    "        rec = float(tp)/(tp+fn)\n",
    "        f_score = (2 * prec * rec)/ (prec + rec)\n",
    "    except:\n",
    "        pass\n",
    "        \n",
    "    f_score_sizes.append(f_score)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(10,4))\n",
    "x = epoch_sizes\n",
    "y = np.array(f_score_sizes) * 100\n",
    "print(y)\n",
    "sns.barplot(x=x, y=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(model.char_vocab) * (model.max_char_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
